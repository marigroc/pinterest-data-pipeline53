{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"/FileStore/tables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out[52]: [FileInfo(path='dbfs:/FileStore/tables/0ed442ca38ad_user_table/', name='0ed442ca38ad_user_table/', size=0, modificationTime=1702749927906),\n",
    " FileInfo(path='dbfs:/FileStore/tables/authentication_credentials.csv', name='authentication_credentials.csv', size=202, modificationTime=1687110999000),\n",
    " FileInfo(path='dbfs:/FileStore/tables/geo_dirty.csv/', name='geo_dirty.csv/', size=0, modificationTime=1702749927906),\n",
    " FileInfo(path='dbfs:/FileStore/tables/pin_dirty.csv/', name='pin_dirty.csv/', size=0, modificationTime=1702749927906),\n",
    " FileInfo(path='dbfs:/FileStore/tables/single_json_file.json', name='single_json_file.json', size=293, modificationTime=1701857903000),\n",
    " FileInfo(path='dbfs:/FileStore/tables/user_dirty.csv/', name='user_dirty.csv/', size=0, modificationTime=1702749927906)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import urllib\n",
    "\n",
    "\"\"\"\n",
    "Read AWS authentication credentials from a CSV file and extract the access key and secret key.\n",
    "\n",
    "Parameters:\n",
    "- spark (SparkSession): The Spark session.\n",
    "- file_path (str): The path to the CSV file containing authentication credentials.\n",
    "\n",
    "Returns:\n",
    "- str: AWS access key.\n",
    "- str: AWS secret key.\n",
    "- str: URL-encoded AWS secret key.\n",
    "\"\"\"\n",
    "# Specify file type to be csv\n",
    "file_type = \"csv\"\n",
    "# Indicates file has first row as the header\n",
    "first_row_is_header = \"true\"\n",
    "# Indicates file has comma as the delimeter\n",
    "delimiter = \",\"\n",
    "# Read the CSV file to spark dataframe\n",
    "aws_keys_df = spark.read.format(file_type)\\\n",
    "    .option(\"header\", first_row_is_header)\\\n",
    "    .option(\"sep\", delimiter)\\\n",
    "    .load(\"dbfs:/FileStore/tables/authentication_credentials.csv\")\n",
    "     \n",
    "\n",
    "# Get the AWS access key and secret key from the spark dataframe\n",
    "ACCESS_KEY = aws_keys_df.where(col('User name')=='databricks-user').select('Access key ID').collect()[0]['Access key ID']\n",
    "SECRET_KEY = aws_keys_df.where(col('User name')=='databricks-user').select('Secret access key').collect()[0]['Secret access key']\n",
    "# Encode the secrete key\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) Spark Jobs\n",
    "Job 979229 View(Stages: 1/1)\n",
    "Job 979242 View(Stages: 1/1)\n",
    "Job 979253 View(Stages: 1/1)\n",
    "aws_keys_df:pyspark.sql.dataframe.DataFrame\n",
    "User name:string\n",
    "Password:string\n",
    "Access key ID:string\n",
    "Secret access key:string\n",
    "Console login link:string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Retrieves a Kinesis stream as a DataFrame using spark.readStream.\n",
    "\n",
    "Parameters:\n",
    "- stream_name (str): The name of the Kinesis stream.\n",
    "\n",
    "Returns:\n",
    "- DataFrame: The Kinesis stream as a DataFrame.\n",
    "\"\"\"\n",
    "def get_stream(stream_name: str):\n",
    "    '''Uses spark.readStream to retrieve Kinesis stream and returns stream as dataframe'''\n",
    "    dataframe = spark \\\n",
    "    .readStream \\\n",
    "    .format('kinesis') \\\n",
    "    .option('streamName', stream_name) \\\n",
    "    .option('initialPosition','earliest') \\\n",
    "    .option('region','us-east-1') \\\n",
    "    .option('awsAccessKey', ACCESS_KEY) \\\n",
    "    .option('awsSecretKey', SECRET_KEY) \\\n",
    "    .load()\n",
    "    return dataframe\n",
    "\n",
    "\"\"\"\n",
    "Deserializes data from a Kinesis stream DataFrame using the provided schema.\n",
    "\n",
    "Parameters:\n",
    "- stream (DataFrame): The Kinesis stream DataFrame.\n",
    "- schema (StructType): The schema to be used for deserialization.\n",
    "\n",
    "Returns:\n",
    "- DataFrame: The deserialized data as a DataFrame.\n",
    "\"\"\"\n",
    "def deserialize_stream(stream, schema):\n",
    "    '''Takes stream dataframe and schema, deserializes data from stream and returns data as dataframe'''\n",
    "    dataframe = stream \\\n",
    "    .selectExpr(\"CAST(data as STRING)\") \\\n",
    "    .withColumn(\"data\", from_json(col(\"data\"), schema)) \\\n",
    "    .select(col(\"data.*\"))\n",
    "    return dataframe\n",
    "\n",
    "\"\"\"\n",
    "Converts matched values in a column of the DataFrame to null based on the specified expression.\n",
    "\n",
    "Parameters:\n",
    "- dataframe (DataFrame): The DataFrame.\n",
    "- column (str): The name of the column to process.\n",
    "- value_to_replace (str): The value to be replaced with null.\n",
    "\n",
    "Returns:\n",
    "- DataFrame: The DataFrame with null values in the specified column.\n",
    "\"\"\"\n",
    "def add_nulls_to_dataframe_column(dataframe, column, value_to_replace):\n",
    "    '''Converts matched values in column of dataframe to null based on expression'''\n",
    "    dataframe = dataframe.withColumn(column, when(col(column).like(value_to_replace), None).otherwise(col(column)))\n",
    "    return dataframe\n",
    "\n",
    "\"\"\"\n",
    "Writes a streaming DataFrame to a Delta table.\n",
    "\n",
    "Parameters:\n",
    "- dataframe (DataFrame): The streaming DataFrame to be written.\n",
    "- name (str): A name string used in options and the table name.\n",
    "\n",
    "Returns:\n",
    "- Data in the Delta table.\n",
    "\"\"\"\n",
    "def write_stream_df_to_table(dataframe, name: str):\n",
    "    '''Takes dataframe and name string and writes dataframe to delta table using name in options and table name'''\n",
    "    dataframe.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", f\"/tmp/kinesis/0abb070c336b_{name}_table_checkpoints/\") \\\n",
    "    .table(f\"0abb070c336b_{name}_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Command took 9.24 seconds -- by mariuszgrocki@duck.com at 17/12/2023, 11:21:47 on Pinterest Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define the schema for the 'pin' DataFrame.\n",
    "\n",
    "Returns:\n",
    "pyspark.sql.types.StructType: The schema for the 'pin' DataFrame.\n",
    "\"\"\"\n",
    "pin_schema = StructType([\n",
    "    StructField(\"index\", IntegerType()),\n",
    "    StructField(\"unique_id\", StringType()),\n",
    "    StructField(\"title\", StringType()),\n",
    "    StructField(\"description\", StringType()),\n",
    "    StructField(\"poster_name\", StringType()),\n",
    "    StructField(\"follower_count\", StringType()),\n",
    "    StructField(\"tag_list\", StringType()),\n",
    "    StructField(\"is_image_or_video\", StringType()),\n",
    "    StructField(\"image_src\", StringType()),\n",
    "    StructField(\"downloaded\", IntegerType()),\n",
    "    StructField(\"save_location\", StringType()),\n",
    "    StructField(\"category\", StringType())\n",
    "])\n",
    "\n",
    "\"\"\"\n",
    "Define the schema for the 'geo' DataFrame.\n",
    "\n",
    "Returns:\n",
    "pyspark.sql.types.StructType: The schema for the 'geo' DataFrame.\n",
    "\"\"\"\n",
    "geo_schema = StructType([\n",
    "    StructField(\"ind\", IntegerType()),\n",
    "    StructField(\"timestamp\", TimestampType()),\n",
    "    StructField(\"latitude\", FloatType()),\n",
    "    StructField(\"longitude\", FloatType()),\n",
    "    StructField(\"country\", StringType())\n",
    "])\n",
    "\n",
    "\"\"\"\n",
    "Define the schema for the 'user' DataFrame.\n",
    "\n",
    "Returns:\n",
    "pyspark.sql.types.StructType: The schema for the 'user' DataFrame.\n",
    "\"\"\"\n",
    "user_schema = StructType([\n",
    "    StructField(\"ind\", IntegerType()),\n",
    "    StructField(\"first_name\", StringType()),\n",
    "    StructField(\"last_name\", StringType()),\n",
    "    StructField(\"age\", StringType()),\n",
    "    StructField(\"date_joined\", TimestampType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Retrieves the Kinesis data stream for Pinterest posts, geolocation data, and user data.\n",
    "\n",
    "Returns:\n",
    "KinesisStream: The Kinesis data streams.\n",
    "\"\"\"\n",
    "pin_stream = get_stream('streaming-0abb070c336b-pin')\n",
    "geo_stream = get_stream('streaming-0abb070c336b-geo')\n",
    "user_stream = get_stream('streaming-0abb070c336b-user')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pin_stream:pyspark.sql.dataframe.DataFrame\n",
    "partitionKey:string\n",
    "data:binary\n",
    "stream:string\n",
    "shardId:string\n",
    "sequenceNumber:string\n",
    "approximateArrivalTimestamp:timestamp\n",
    "geo_stream:pyspark.sql.dataframe.DataFrame\n",
    "partitionKey:string\n",
    "data:binary\n",
    "stream:string\n",
    "shardId:string\n",
    "sequenceNumber:string\n",
    "approximateArrivalTimestamp:timestamp\n",
    "user_stream:pyspark.sql.dataframe.DataFrame\n",
    "partitionKey:string\n",
    "data:binary\n",
    "stream:string\n",
    "shardId:string\n",
    "sequenceNumber:string\n",
    "approximateArrivalTimestamp:timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Deserialiselize a streaming DataFrame from a specified stream using the provided schema.\n",
    "\n",
    "Parameters:\n",
    "- stream (str): The name of the stream to deserialize.\n",
    "- schema (StructType): The schema to apply to the deserialized DataFrame.\n",
    "\n",
    "Returns:\n",
    "- DataFrame: The deserialised DataFrame.\n",
    "\"\"\"\n",
    "df_pin = deserialize_stream(pin_stream, pin_schema)\n",
    "df_geo = deserialize_stream(geo_stream, geo_schema)\n",
    "df_user = deserialize_stream(user_stream, user_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_pin:pyspark.sql.dataframe.DataFrame\n",
    "index:integer\n",
    "unique_id:string\n",
    "title:string\n",
    "description:string\n",
    "poster_name:string\n",
    "follower_count:string\n",
    "tag_list:string\n",
    "is_image_or_video:string\n",
    "image_src:string\n",
    "downloaded:integer\n",
    "save_location:string\n",
    "category:string\n",
    "df_geo:pyspark.sql.dataframe.DataFrame\n",
    "ind:integer\n",
    "timestamp:timestamp\n",
    "latitude:float\n",
    "longitude:float\n",
    "country:string\n",
    "df_user:pyspark.sql.dataframe.DataFrame\n",
    "ind:integer\n",
    "first_name:string\n",
    "last_name:string\n",
    "age:string\n",
    "date_joined:timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The code performs a series of data cleaning and transformation operations on the DataFrame df_pin.\n",
    "\n",
    "1. Replace empty entries and entries with no relevant data in specific columns with None.\n",
    "   - Columns and values for replacement are defined in the dictionary columns_and_values_for_null.\n",
    "\n",
    "2. Perform necessary transformations on the 'follower_count' column to ensure every entry is a number.\n",
    "   - Replace 'k' with '000' and 'M' with '000000'.\n",
    "\n",
    "3. Cast selected numeric columns to the correct data type ('double').\n",
    "   - Numeric columns are specified in the list numeric_columns.\n",
    "\n",
    "4. Modify the 'save_location' column to include only the saved location path.\n",
    "   - Remove the prefix 'Local save in '.\n",
    "\n",
    "5. Rename the 'index' column to 'ind'.\n",
    "\n",
    "6. Reorder the DataFrame columns to the desired sequence specified in the new_order list.\n",
    "\n",
    "7. Display the changes using df_pin.show().\n",
    "\n",
    "Note: The add_nulls_to_dataframe_column function is assumed to be defined elsewhere in the codebase.\n",
    "\"\"\"\n",
    "\n",
    "# Columns and values to change to nulla\n",
    "columns_and_values_for_null = {\n",
    "    \"description\": \"No description available%\",\n",
    "    \"follower_count\": \"User Info Error\",\n",
    "    \"image_src\": \"Image src error.\",\n",
    "    \"poster_name\": \"User Info Error\",\n",
    "    \"tag_list\": \"N,o, ,T,a,g,s, ,A,v,a,i,l,a,b,l,e\",\n",
    "    \"title\": \"No Title Data Available\"\n",
    "}\n",
    "# Loop through dictionary, calling function with dictionary values as arguments\n",
    "for key, value in columns_and_values_for_null.items():\n",
    "    df_pin = add_nulls_to_dataframe_column(df_pin, key, value)\n",
    "# Perform the necessary transformations on the follower_count to ensure every entry is a number\n",
    "df_pin = df_pin.withColumn(\"follower_count\", regexp_replace(\"follower_count\", \"k\", \"000\"))\n",
    "df_pin = df_pin.withColumn(\"follower_count\", regexp_replace(\"follower_count\", \"M\", \"000000\"))\n",
    "# cast follower_count column to integer type\n",
    "df_pin = df_pin.withColumn(\"follower_count\", col(\"follower_count\").cast('int'))\n",
    "# convert save_location column to include only the save location path\n",
    "df_pin = df_pin.withColumn(\"save_location\", regexp_replace(\"save_location\", \"Local save in \", \"\"))\n",
    "# rename the index column to ind\n",
    "df_pin = df_pin.withColumnRenamed(\"index\", \"ind\")\n",
    "# reorder columns\n",
    "new_pin_column_order = [\n",
    "    \"ind\",\n",
    "    \"unique_id\",\n",
    "    \"title\",\n",
    "    \"description\",\n",
    "    \"follower_count\",\n",
    "    \"poster_name\",\n",
    "    \"tag_list\",\n",
    "    \"is_image_or_video\",\n",
    "    \"image_src\",\n",
    "    \"save_location\",\n",
    "    \"category\"\n",
    "]\n",
    "df_pin = df_pin.select(new_pin_column_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_pin:pyspark.sql.dataframe.DataFrame\n",
    "ind:integer\n",
    "unique_id:string\n",
    "title:string\n",
    "description:string\n",
    "follower_count:integer\n",
    "poster_name:string\n",
    "tag_list:string\n",
    "is_image_or_video:string\n",
    "image_src:string\n",
    "save_location:string\n",
    "category:string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Transforms the user DataFrame with the following steps:\n",
    "\n",
    "1. Create a new column for the full name by concatenating 'first_name' and 'last_name'.\n",
    "2. Drop the 'first_name' and 'last_name' columns.\n",
    "3. Convert the 'date_joined' column to timestamp data type.\n",
    "4. Define the desired new column order.\n",
    "5. Reorder the DataFrame columns to enforce the new order.\n",
    "6. Display the updated DataFrame.\n",
    "7. Print the schema changes.\n",
    "\n",
    "Parameters:\n",
    "- df_user (DataFrame): The original user DataFrame.\n",
    "\n",
    "Returns:\n",
    "- df_user (DataFrame): The transformed user DataFrame.\n",
    "\"\"\"\n",
    "\n",
    "# import types\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "# define function for returning list containing two values\n",
    "def combine_lat_and_long(latitude, longitude):\n",
    "    return [latitude, longitude]\n",
    "# define new user-defined function\n",
    "new_func = udf(combine_lat_and_long, ArrayType(DoubleType()))\n",
    "# apply new udf to combine latitude and longitude columns\n",
    "df_geo = df_geo.withColumn(\"coordinates\", new_func(\"latitude\", \"longitude\"))\n",
    "# drop the latitude and longitude columns\n",
    "cols_to_drop = (\"latitude\", \"longitude\")\n",
    "df_geo = df_geo.drop(*cols_to_drop)\n",
    "# convert timestamp column from type string to type timestamp\n",
    "df_geo = df_geo.withColumn(\"timestamp\", to_timestamp(\"timestamp\"))\n",
    "# change column order\n",
    "new_geo_column_order = [\n",
    "    \"ind\",\n",
    "    \"country\",\n",
    "    \"coordinates\",\n",
    "    \"timestamp\",\n",
    "]\n",
    "df_geo = df_geo.select(new_geo_column_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_geo:pyspark.sql.dataframe.DataFrame\n",
    "ind:integer\n",
    "country:string\n",
    "coordinates:array\n",
    "element:double\n",
    "timestamp:timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Transforms the user DataFrame with the following steps:\n",
    "\n",
    "1. Create a new column for the full name by concatenating 'first_name' and 'last_name'.\n",
    "2. Drop the 'first_name' and 'last_name' columns.\n",
    "3. Convert the 'date_joined' column to timestamp data type.\n",
    "4. Define the desired new column order.\n",
    "5. Reorder the DataFrame columns to enforce the new order.\n",
    "6. Display the updated DataFrame.\n",
    "7. Print the schema changes.\n",
    "\n",
    "Parameters:\n",
    "- df_user (DataFrame): The original user DataFrame.\n",
    "\n",
    "Returns:\n",
    "- df_user (DataFrame): The transformed user DataFrame.\n",
    "\"\"\"\n",
    "# create new column for full name\n",
    "df_user = df_user.withColumn(\"user_name\", concat_ws(\" \", \"first_name\", \"last_name\"))\n",
    "# drop the first_name and last_name columns\n",
    "cols_to_drop = (\"first_name\", \"last_name\")\n",
    "df_user = df_user.drop(*cols_to_drop)\n",
    "# convert date_joined column from type string to type timestamp\n",
    "df_user = df_user.withColumn(\"date_joined\", to_timestamp(\"date_joined\"))\n",
    "# change column order\n",
    "new_user_column_order = [\n",
    "    \"ind\",\n",
    "    \"user_name\",\n",
    "    \"age\",\n",
    "    \"date_joined\",\n",
    "]\n",
    "df_user = df_user.select(new_user_column_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_user:pyspark.sql.dataframe.DataFrame\n",
    "ind:integer\n",
    "user_name:string\n",
    "age:string\n",
    "date_joined:timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Process dataframes and display them.\n",
    "\n",
    "Parameters:\n",
    "- df_pin (DataFrame): DataFrame for Pinterest data.\n",
    "- df_geo (DataFrame): DataFrame for geolocation data.\n",
    "- df_user (DataFrame): DataFrame for user data.\n",
    "\n",
    "Returns:\n",
    "None\n",
    "\"\"\"\n",
    "display(df_pin)\n",
    "display(df_geo)\n",
    "display(df_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "display_query_572(id: f73688fa-782a-4f67-9ca2-ff55a3af7bc9)\n",
    "Last updated: 10 seconds ago\n",
    "display_query_571(id: 1f3344a0-4b4f-485a-9ea1-c2da124ce630)\n",
    "Last updated: 5 seconds ago\n",
    "display_query_570(id: 062cb61b-d1d1-43ed-95a0-d300d75c76de)\n",
    "Last updated: 5 seconds ago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind  user_name     age  date_joined\n",
    "7528 Abigail Ali   20   2015-10-24T11:23:51.000+0000\n",
    "2863 Dylan Holmes  32   2016-10-23T14:06:51.000+0000\n",
    "5730 Rachel Davis  36   2015-12-08T20:02:43.000+0000\n",
    "8304 Charles Berry 25   2015-12-28T04:21:39.000+0000\n",
    "\n",
    "\n",
    "ind   unique_id                           title    description\n",
    "\n",
    "7528 fbe53c66-3442-4773-b19e-d3ec6f54dddf null     null\n",
    "2863 9bf39437-42a6-4f02-99a0-9a0383d8cd70 25       Super Fun Summer Crafts for Kids - Of Life and Lisa\n",
    "Keep the kids busy this summer with these easy diy crafts and projects. Creative and…\n",
    "\n",
    "5730 1e1f0c8b-9fcf-460b-9154-c775827206eb Island Oasis Coupon Organizer  Description Coupon Organizer in a fun colorful fabric -island oasis, Great Size for the \"basic\" couponer - holds up to 500 coupons with ease, and is made long enough so that you… \n",
    "8304 5b6d0913-25e4-43ab-839d-85d5516f78a4 The #1 Reason You’re Not His Priority Anymore - Matthew Coast #lovequotes #matchmaker #matchmadeinheaven #loveyourself #respectyourself\n",
    "\n",
    "ind  country  coordinates                                timestamp\n",
    "\n",
    "7528 Albania  [-89.97869873046875, -173.29299926757812]  2020-08-28T03:52:47.000+0000\n",
    "2863 Armenia  [-5.344449996948242, -177.9239959716797]   2020-04-27T13:34:16.000+0000\n",
    "5730 Colombia [-77.01499938964844, -101.43699645996094]  2021-04-19T17:37:03.000+0000\n",
    "8304 French Guiana [-28.88520050048828, -164.8699951171875] 2019-09-13T04:50:29.000+0000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write the streaming DataFrame to a Delta table.\n",
    "\n",
    "Parameters:\n",
    "- dataframe (pyspark.sql.DataFrame): The streaming DataFrame to be written to the Delta table.\n",
    "- table_name (str): The name of the Delta table to be created.\n",
    "\n",
    "Returns:\n",
    "None\n",
    "\"\"\"\n",
    "\n",
    "write_stream_df_to_table(df_pin, \"pin\")\n",
    "write_stream_df_to_table(df_geo, \"geo\")\n",
    "write_stream_df_to_table(df_user, \"user\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
